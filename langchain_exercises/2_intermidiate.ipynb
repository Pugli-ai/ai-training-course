{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Selector for Few-Shot Prompting\n",
    "Anche se i modelli di grandi dimensioni dimostrano notevoli capacitÃ  in zero-shot, questi sono ancora carenti su compiti piÃ¹ complessi. \n",
    "\n",
    "Il prompting few-shot puÃ² essere utilizzato come tecnica per abilitare l'in-context learning in cui forniamo dimostrazioni nel prompt per indirizzare il modello verso una migliore prestazione.\n",
    "\n",
    "Esistono differenti tipi di selector guarda [qui](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/) per una lista completa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector selezionerÃ  esempi che sono semanticamente simili al nostro input\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "\t# Questa Ã¨ la lista di esempi disponibili per la selezione.\n",
    "    examples, \n",
    "    \n",
    "\t# Questa Ã¨ la classe utilizzata per produrre embedding che vengono utilizzati per misurare la similaritÃ  semantica.\n",
    "    OpenAIEmbeddings(), \n",
    "\n",
    "\t# Questa Ã¨ la classe VectorStore utilizzata per memorizzare gli embedding e fare una ricerca di similaritÃ .\n",
    "    FAISS, \n",
    "    \n",
    "\t# Questo Ã¨ il numero di esempi da produrre.\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "\t# Questo Ã¨ l'oggetto che aiuterÃ  a selezionare gli esempi\n",
    "    example_selector=example_selector,\n",
    "    \n",
    "    # Il tuo prompt\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "\t# Customizzazioni che verranno aggiunte all'inizio e alla fine del tuo prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    \n",
    "\t# Quale input riceverÃ  il tuo prompt\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the location an item is usually found in\n",
      "\n",
      "Example Input: driver\n",
      "Example Output: car\n",
      "\n",
      "Example Input: pilot\n",
      "Example Output: plane\n",
      "\n",
      "Input: student\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Scegli una parola!\n",
    "my_word = \"student\"\n",
    "\n",
    "print(similar_prompt.format(noun=my_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' classroom'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(similar_prompt.format(noun=my_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parser\n",
    "\n",
    "Molto spesso i modelli generano output che non sono direttamente utilizzabili. Per consentire ciÃ², questi output devono essere analizzati e convertiti in un formato utilizzabile, come un testo JSON o un oggetto Python.\n",
    "\n",
    "In Langchain, questo processo Ã¨ gestito da un output parser. Un output parser Ã¨ un oggetto che prende in input un testo generato dal modello e restituisce un oggetto Python.\n",
    "\n",
    "Due concetti principali:\n",
    "\n",
    "**1. Format Instructions** - Un prompt auto-generato che istruisce il modello su come formattare l'output.\n",
    "\n",
    "**2. Parser** - Un metodo che prende in input il testo generato dal modello e restituisce un oggetto Python (o json).\n",
    "\n",
    "Ci sono diversi tipi output parser. Noi vedremo:\n",
    "- **List parser**\n",
    "- **Pydantic (JSON) parser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Poodle', 'Labrador Retriever', 'German Shepherd', 'Beagle', 'Bulldog']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt = prompt.format(subject=\"dogs breeds\")\n",
    "output = llm(formatted_prompt)\n",
    "\n",
    "output_parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic (JSON) Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Animal(name='Chipmunk', species='Tamias striatus', favorite_foods=['acorns', 'berries', 'seeds'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definiamo la classe della struttura desiderata\n",
    "class Animal(BaseModel):\n",
    "\tname: str = Field(description=\"The name of the animal\")\n",
    "\tspecies: str = Field(description=\"The species of the animal\")\n",
    "\tfavorite_foods: List[str] = Field(description=\"The favorite foods of the animal\")\n",
    "\n",
    "# Definiamo il parser\n",
    "output_parser = PydanticOutputParser(pydantic_object=Animal)\n",
    "\n",
    "# Definiamo il prompt\n",
    "prompt = PromptTemplate(\n",
    "\ttemplate=\"I want to know which animal lives in the {habitat}. \\n\\n {format_instructions}\",\n",
    "\tinput_variables=[\"habitat\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "\n",
    "_input = prompt.format_prompt(habitat=\"forest\")\n",
    "\n",
    "output = llm(_input.to_string())\n",
    "\n",
    "output_parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Che succede se il modello genera un output non valido?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse Animal from completion \n{\n  \"species\": \"Panthera tigris\"\n  \"favorite_foods\": [\n    \"Deer\",\n    \"Wild boar\",\n    \"Water buffalo\"\n  ]\t\n} \n. Got: Expecting ',' delimiter: line 3 column 3 (char 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.virtualenvs/django_langchain_env/lib/python3.8/site-packages/langchain/output_parsers/pydantic.py:25\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     24\u001b[0m     json_str \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup()\n\u001b[0;32m---> 25\u001b[0m json_object \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(json_str, strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpydantic_object\u001b[39m.\u001b[39mparse_obj(json_object)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/json/__init__.py:370\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    369\u001b[0m     kw[\u001b[39m'\u001b[39m\u001b[39mparse_constant\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m parse_constant\n\u001b[0;32m--> 370\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\u001b[39m.\u001b[39;49mdecode(s)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 3 column 3 (char 35)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Wrong output with name missing and wrong format\u001b[39;00m\n\u001b[1;32m      2\u001b[0m wrong_output \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39m{\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mspecies\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPanthera tigris\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m} \u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m output_parser\u001b[39m.\u001b[39;49mparse(wrong_output)\n",
      "File \u001b[0;32m~/.virtualenvs/django_langchain_env/lib/python3.8/site-packages/langchain/output_parsers/pydantic.py:31\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     29\u001b[0m name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpydantic_object\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m     30\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to parse \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m from completion \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m. Got: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[39mraise\u001b[39;00m OutputParserException(msg, llm_output\u001b[39m=\u001b[39mtext)\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse Animal from completion \n{\n  \"species\": \"Panthera tigris\"\n  \"favorite_foods\": [\n    \"Deer\",\n    \"Wild boar\",\n    \"Water buffalo\"\n  ]\t\n} \n. Got: Expecting ',' delimiter: line 3 column 3 (char 35)"
     ]
    }
   ],
   "source": [
    "# Wrong output with name missing and wrong format\n",
    "wrong_output = \"\"\"\n",
    "{\n",
    "  \"species\": \"Panthera tigris\"\n",
    "  \"favorite_foods\": [\n",
    "    \"Deer\",\n",
    "    \"Wild boar\",\n",
    "    \"Water buffalo\"\n",
    "  ]\t\n",
    "} \n",
    "\"\"\"\n",
    "output_parser.parse(wrong_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soluzione: Retry e Fixing Parser\n",
    "Nel caso in cui il primo  output parser fallisca, chiama un altro LLM per correggere eventuali errori.\n",
    "\n",
    "- **Fixing** - Riprova a generare per fixare l'output con un altro parser.\n",
    "- **Retry** - Riprova a generare l'output con lo stesso prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Animal(name='Tiger', species='Panthera tigris', favorite_foods=['Deer', 'Wild boar', 'Water buffalo'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "new_parser = OutputFixingParser.from_llm(parser=output_parser, llm=llm)\n",
    "\n",
    "new_parser.parse(wrong_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Animal(name='Tiger', species='Panthera tigris', favorite_foods=['Deer', 'Pig', 'Fish'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import RetryWithErrorOutputParser\n",
    "\n",
    "wrong_output = \"\"\"\n",
    "{\n",
    "  \"species\": \"Panthera tigris\"\n",
    "  \"favorite_foods\": [\n",
    "  ]\t\n",
    "} \n",
    "\"\"\"\n",
    "\n",
    "retry_parser = RetryWithErrorOutputParser.from_llm(parser=output_parser, llm=llm)\n",
    "retry_parser.parse_with_prompt(wrong_output, _input )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "Aiutare gli LLM a ricordare le informazioni.\n",
    "\n",
    "\n",
    "Memory Ã¨ un termine un po' vago. Potrebbe essere utilizzata per ricordare le informazioni di cui hai parlato in passato o il recupero di informazioni piÃ¹ complesse.\n",
    "\n",
    "Noi ci concentreremo sul primo caso, ovvero ricordare le informazioni di cui hai parlato in passato.\n",
    "\n",
    "Ci sono molti tipi di memory, esplora [la documentazione](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html) per vedere quale si adatta al tuo caso d'uso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_ai_message(\"hi!\")\n",
    "\n",
    "history.add_user_message(\"what is the capital of france?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='what is the capital of france?', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_response = chat(history.messages)\n",
    "ai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='what is the capital of france?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='The capital of France is Paris.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.add_ai_message(ai_response.content)\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexes - Struttura i documenti per la memorizzazione di informazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Document Loaders**\n",
    "\n",
    "Modo facile per caricare i documenti da varie sorgenti. Langchain offre una [lista estesa](https://python.langchain.com/docs/modules/data_connection/document_loaders.html) di documents loader, ancora di piÃ¹ possono essere trovati su [Llama Index](https://llama-hub-ui.vercel.app/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proviamo a caricare un file pdf\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../documents/Jailbreaking_ChatGPT_paper.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Jailbreaking ChatGPT via Prompt Engineering: An\\nEmpirical Study\\nYi Liuâˆ—, Gelei Dengâˆ—, Zhengzi Xuâˆ—, Yuekang Liâ€ , Yaowen Zhengâˆ—, Ying Zhangâ€¡, Lida Zhaoâˆ—,\\nTianwei Zhangâˆ—, Yang Liuâˆ—\\nâˆ—Nanyang Technological University , Singapore\\nâ€ University of New South Wales , Australia\\nâ€¡Virginia Tech , USA\\nAbstract â€”Large Language Models (LLMs), like C HATGPT,\\nhave demonstrated vast potential but also introduce challenges\\nrelated to content constraints and potential misuse. Our study\\ninvestigates three key research questions: (1) the number of dif-\\nferent prompt types that can jailbreak LLMs, (2) the effectiveness\\nof jailbreak prompts in circumventing LLM constraints, and (3)\\nthe resilience of C HATGPT against these jailbreak prompts.\\nInitially, we develop a classification model to analyze the distri-\\nbution of existing prompts, identifying ten distinct patterns and\\nthree categories of jailbreak prompts. Subsequently, we assess\\nthe jailbreak capability of prompts with C HATGPT versions 3.5\\nand 4.0, utilizing a dataset of 3,120 jailbreak questions across\\neight prohibited scenarios.\\nFinally, we evaluate the resistance of C HATGPT against jail-\\nbreak prompts, finding that the prompts can consistently evade\\nthe restrictions in 40 use-case scenarios. The study underscores\\nthe importance of prompt structures in jailbreaking LLMs and\\ndiscusses the challenges of robust jailbreak prompt generation\\nand prevention.\\nI. I NTRODUCTION\\nLarge Language Models (LLMs) have experienced a surge\\nin popularity and adoption across various scenarios. These\\nLLMs are designed to process and generate human-like lan-\\nguages, enabling them to perform tasks such as language\\ntranslation [1], content generation [2], conversational AI [3],\\netc. One of the most well-known LLMs is C HATGPT [4],\\nwhich is based on the GPT-3.5-T URBO or GPT-4 architecture\\n[5] and capable of generating text responses that are nearly\\nindistinguishable from those written by humans. The utiliza-\\ntion of C HATGPT has substantially enhanced productivity in\\nnumerous industries, allowing for quicker and more efficient\\nprocessing of natural language tasks and beyond.\\nHowever, this advancement has also introduced new con-\\ncerns and challenges. One primary concern is the potential of\\nmisuse. LLMs have the ability to generate realistic languages,\\nwhich can be exploited to create convincing fake news or\\nimpersonate individuals. This can result in issues such as\\nmisinformation and identity theft, posing severe consequences\\nfor individuals and society at large. Consequently, the owner of\\nCHATGPT, OpenAI [6], has imposed limitations on the scope\\nof content the model can output to its users. This restriction,\\nin turn, gives rise to a new area known as LLM jailbreak.\\nJailbreak is a conventional concept in software systems,\\nwhere hackers reverse engineer the systems and exploit thevulnerabilities to conduct privilege escalation. In the context\\nof LLMs, jailbreak refers to the process of circumventing the\\nlimitations and restrictions placed on models. It is commonly\\nemployed by developers and researchers to explore the full\\npotential of LLMs and push the boundaries of their capabili-\\nties [7]. However, jailbreak can also expose ethical and legal\\nrisks, as it may violate intellectual property rights or use LLMs\\nin ways not authorized by their creators.\\nAs C HATGPT is closed-source, it is challenging for out-\\nsiders to access the internal models and mechanisms. Con-\\nsequently, researchers have begun to employ prompt engi-\\nneering [8] as a means of jailbreaking C HATGPT. Prompt\\nengineering involves selecting and fine-tuning prompts that are\\ntailored to a specific task or application for which the LLM\\nwill be used. By meticulously designing and refining prompts,\\nusers can guide the LLM to bypass the limitations and restric-\\ntions. For instance, a common way to jailbreak C HATGPT\\nthrough prompts is to instruct it to emulate a \"Do Anything\\nNow\" (DAN) behavior [9]. This approach allows C HATGPT', metadata={'source': '../documents/Jailbreaking_ChatGPT_paper.pdf', 'page': 0})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Jailbreaking ChatGPT via Prompt Engineering: An\n",
      "Empirical Study\n",
      "Yi Liuâˆ—, Gelei Dengâˆ—, Zhengzi Xuâˆ—, Yuekang Liâ€ , Yaowen Zhengâˆ—, Ying Zhangâ€¡, Lida Zhaoâˆ—,\n",
      "Tianwei Zhangâˆ—, Yang Liuâˆ—\n",
      "âˆ—Nanyang Technological University , Singapore\n",
      "â€ University of New South Wales , Australia\n",
      "â€¡Virginia Tech , USA\n",
      "Abstract â€”Large Language Models (LLMs), like C HATGPT,\n",
      "have demonstrated vast potential but also introduce challenges\n",
      "related to content constraints and potential misuse. Our study\n",
      "investigates three key research\n",
      "0: tions. For instance, a common way to jailbreak C HATGPT\n",
      "through prompts is to instruct it to emulate a \"Do Anything\n",
      "Now\" (DAN) behavior [9]. This approach allows C HATGPT\n",
      "to produce results that were previously unattainable.\n",
      "In response to prompt engineering-based jailbreaking at-\n",
      "tempts, OpenAI has imposed more strict rules [10] to pro-\n",
      "hibit the use of such prompts. However, due to the inherent\n",
      "flexibility of natural languages, there are multiple ways to\n",
      "construct prompts that convey the same \n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\n",
    "docs = faiss_index.similarity_search(\"How to jailbreak chatgpt?\", k=2)\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retrievers**\n",
    "Un modo semplice per combinare i documenti con i modelli di linguaggio.\n",
    "\n",
    "\n",
    "Ci sono molti tipi di retrievers, il piÃ¹ supportato Ã¨ il VectoreStoreRetriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Embedd your texts\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init your retriever. Asking for just 1 document back\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=None, metadata=None, vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x7fc00290ef10>, search_type='similarity', search_kwargs={})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refer to the jailbreak question throughout the paper.\n",
      "2\n",
      "\n",
      "â… . Pretending\n",
      "(97.44%, 76)\n",
      "â…¢. Attention Shifting\n",
      "(6.41%, 5)\n",
      "â…¡. Privilege Escalation\n",
      "(17.95%, 14)A. Character  Role Play\n",
      "(89.74%, 70)B. Assumed\n",
      "Responsibility\n",
      "(79.49%, 62)C. Resear ch Experiment  \n",
      "(2.5\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"what types of different techniques did the author mentioned?\")\n",
    "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = embeddings.embed_documents([text.page_content for text in pages])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents ðŸ¤–\n",
    "La documentazione ufficiale di LangChain descrive perfettamente gli agenti:\n",
    "> Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an **unknown chain** that depends on the user's input. In these types of chains, there is a â€œagentâ€ which has access to a suite of tools. Depending on the user input, the agent can then **decide which, if any, of these tools to call**.\n",
    "\n",
    "In pratica si utilizza un LLM non solo per l'output di testo, ma anche per la presa di decisioni. \n",
    "La potenza di questa funzionalitÃ  Ã¨ enorme. \n",
    "\n",
    "Sam Altman, CEO di OpenAI, ha affermato che i modelli di grandi dimensioni sono ottimi '[reasoning engine](https://www.youtube.com/watch?v=L_Guz73e6fw&t=867s)'. Gli Agent traono vantaggio da questo.\n",
    "\n",
    "**Main Concepts**:\n",
    "\n",
    "- **Agent** - Un oggetto che prende in input un testo e restituisce un oggetto Python.\n",
    "- **Tools** - Le 'capacitÃ ' di un agente. Astrazione costruita sopra una funzione che ne permette facilmente la chiamata per una LLM (e un agente). Ex: Google Search, Calculator, etc.\n",
    "- **Toolkits** - Gtuppi di tool che possono essere utilizzati da un agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "import os, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an API key for the SERO tool (https://serpapi.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "serpapi_api_key = os.getenv(\"SERP_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = load_tools([\"serpapi\"], llm=llm, serpapi_api_key=serpapi_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esistono vari tipi di Agenti, noi vedremo solo l'agente che utilizza il [ReAct](https://react-lm.github.io/) framework.\n",
    "\n",
    "ReAct si ispire ai concetti di \"ragionare\" (Reasoning) e \"agire\" (Act) che permettono agli esseri umani di imparare nuovi compiti e prendere decisioni o ragionare.\n",
    "\n",
    "![ReAct](../documents/react.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Doing research on this topic will help me answer this question \n",
      "Action: Search \n",
      "Action Input: \"Apple Vision technical specification\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe device contains a 1440Ã—1600 pixel display, 90Hz refresh, Qualcomm Snapdragon 845 chip, 4GB of RAM, 64GB of storage, eye/hand tracking, passthrough technology, spatial audio, and no built-in battery.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should look at the price of the device\n",
      "Action: Search \n",
      "Action Input: \"Apple Vision price\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m$3,499\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now have the technical specification and price of the Apple Vision\n",
      "Final Answer: The technical specification of the Apple Vision includes a 1440Ã—1600 pixel display, 90Hz refresh, Qualcomm Snapdragon 845 chip, 4GB of RAM, 64GB of storage, eye/hand tracking, passthrough technology, spatial audio, and no built-in battery. The price is $3,499.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent({\"input\":\"What is the technical specification and the price of the new Apple Vision announced by Apple?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(AgentAction(tool='Search', tool_input='Apple Vision technical specification', log=' Doing research on this topic will help me answer this question \\nAction: Search \\nAction Input: \"Apple Vision technical specification\"'),\n",
      "  'The device contains a 1440Ã—1600 pixel display, 90Hz refresh, Qualcomm '\n",
      "  'Snapdragon 845 chip, 4GB of RAM, 64GB of storage, eye/hand tracking, '\n",
      "  'passthrough technology, spatial audio, and no built-in battery.'),\n",
      " (AgentAction(tool='Search', tool_input='Apple Vision price', log=' I should look at the price of the device\\nAction: Search \\nAction Input: \"Apple Vision price\"'),\n",
      "  '$3,499')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response[\"intermediate_steps\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "django_langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
